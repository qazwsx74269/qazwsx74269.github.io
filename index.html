<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    

    <!--Author-->
    
        <meta name="author" content="Zehan Song">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Zehan Song&#39;s Blog"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Zehan Song&#39;s Blog"/>

    <!--Type page-->
    
        <meta property="og:type" content="website" />
    

    <!--Page Cover-->
    

    <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>Zehan Song&#39;s Blog</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Google Analytics -->
    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-117527815-1', 'auto');
        ga('send', 'pageview');

    </script>



</head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about-me">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
            <h1 id="main-title" class="title">Zehan Song's Blog</h1>
        
    </div>
</header>

        <section class="main">
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/08/05/survey-about-video-visual-relation-detection-in-computer-vision/">
                survey-about-video-visual-relation-detection-in-computer-vision
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-08-05</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h1><h2 id="Video-object-detection"><a href="#Video-object-detection" class="headerlink" title="Video object detection"></a>Video object detection</h2><ol>
<li><p>Kai Kang, Wanli Ouyang, Hongsheng Li, and Xiaogang Wang. 2016. Object detection from video tubelets with convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 817–825.<br>通过联合针对静态图像的物体检测方法和一般物体跟踪方法，从给定的视频中产生    许多小段的物体跟踪提议，来预测小段中含有物体的概率。</p>
</li>
<li><p>Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 779–788.<br>将图像网格化，对每一个网格进行物体的·bounding box和类别的预测，最后进行    NMS处理，建立了一个端到端的模型。</p>
</li>
<li><p>Wenhan Luo, Junliang Xing, Xiaoqin Zhang, Xiaowei Zhao, and Tae-Kyun . 2014. Multiple object tracking: A literature review. arXiv:1409.7618 (2014).<br>本文描述了视频中多目标跟踪（MOT）任务的相关方法和问题，提出了一个统一问题公式和一些现有方法的分类方式，介绍了state-of-the-art MOT算法的关键因素，并讨论了MOT算法的测评包括评价指标、公开数据集，开源代码的实现，和基准测试结果。</p>
</li>
<li><p>Long Ying, Tianzhu Zhang, and Changsheng Xu. 2015. Multi-object tracking via MHT with multiple information fusion in surveillance video. Multimedia Systems 21, 3 (2015), 313–326.<br>本文提出了一个基于多信息融合的多假设追踪算法，涉及到HSV-LBP表观特征、局    部动作模式以及排斥惯性模型。</p>
</li>
<li><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 770–778.<br>提出了一个残差网络框架，有利于训练更深的网络，使得图像的识别准确率得到提升。</p>
</li>
<li><p>Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network. arXiv:1312.4400 (2013).<br>提出MLP卷积层和全局平均池化，改进了传统的CNN网络，减少了需要训练的网络参    数。</p>
</li>
<li><p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems. IEEE, 91–99.<br>Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box     regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大    提高，在检测速度方面尤为明显。</p>
</li>
<li><p>Meng Wang, Changzhi Luo, Richang Hong, Jinhui Tang, and Jiashi Feng. 2016. Beyond object proposals: Random crop pooling for multi-label image recognition. IEEE Transactions on Image Processing 25, 12 (2016), 5678–5688.<br>针对多标签图像识别任务，提出一个随机剪切的池化方法，不需要预先产生大量的    object proposals，也不需要后处理步骤。</p>
</li>
<li><p>Wongun Choi. 2015. Near-online multi-target tracking with aggregated local flow descriptor. In IEEE International Conference on Computer Vision. IEEE, 3029–3037.<br>本文解决了MOT的两个关键挑战，一是如何准确地衡量两个检测之间的相似性，    二是如何有效地把全局跟踪的算法思想用至在线应用中。</p>
</li>
<li><p>Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, Mi Young Nam, Young Giu Jung,  and Phill Kyu Rhee. 2016. Multi-class Multi-object Tracking Using ChangingPoint Detection. In European Conference on Computer Vision. Springer, 68–83.<br>本文提出了一种新颖的多类别多物体跟踪框架，使用变点检测模型来检测突变和异    常。</p>
</li>
<li><p>Anton Milan, Stefan Roth, and Konrad Schindler. 2014. Continuous energy minimization for multitarget tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 36, 1 (2014), 58–72.<br>该论文从运动的整体性出发，提出了一个整体的，比较贴合运动特征的能量函数，然后通过对该能量函数进行寻优得到较好的跟踪结果。</p>
</li>
<li><p>Dan Oneata, Jérôme Revaud, Jakob Verbeek, and Cordelia Schmid. 2014. Spatiotemporal object detection proposals. In European conference on computer vision. Springer, 737–752.<br>本文提出基于超体素单元的合并来获得视频的细管状提议，又提出一种新颖的产生超体素的方法，从对每帧所提取的超像素进行结构化聚类开始进行。</p>
</li>
<li><p>Xindi Shang, Tongwei Ren, Hanwang Zhang, Gangshan Wu, and Tat-Seng Chua.2017. Object trajectory proposal. In IEEE International Conference on Multimedia and Expo. IEEE.<br>本文提出一种产生物体轨迹提议的方法，也就是将移动物体与静止物体区分开来分    别产生轨迹提议，然后进行统一评价排序，合并最优结果。</p>
</li>
</ol>
<h2 id="Visual-relation-detection"><a href="#Visual-relation-detection" class="headerlink" title="Visual relation detection"></a>Visual relation detection</h2><ol start="14">
<li><p>Bo Dai, Yuqi Zhang, and Dahua Lin. 2017. Detecting Visual Relationships With Deep Relational Networks. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE.<br>本文针对视觉关系检测任务提出了一个新的框架，综合考虑表观特征、空间结构以及主宾与谓词间的统计依赖，分别预测主谓宾。</p>
</li>
<li><p>Yikang Li, Wanli Ouyang, Xiaogang Wang, and Xiao’ou Tang. 2017. ViP-CNN: Visual Phrase Guided Convolutional Neural Network. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE.<br>本文提出了一种以视觉短语为指导的消息传递结构，更好地对各个预测分支模型之间的相互依赖关系进行建模。</p>
</li>
<li><p>Xiaodan Liang, Lisa Lee, and Eric P. Xing. 2017. Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE.<br>本文通过对图片中出现的物体、关系、属性构建全局语义图，运用一种变化结构化的遍历策略和增强学习来实现模型的运作。</p>
</li>
<li><p>Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. 2016. Visual relationship detection with language priors. In European Conference on Computer Vision. Springer, 852–869.<br>本文通过利用关系中的主谓宾的语义信息和表观特征来进行建模，并引入了新的数据集Visual Relationship Dataset。</p>
</li>
<li><p>Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and Tat-Seng Chua. 2017. Visual Translation Embedding Network for Visual Relation Detection. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE.<br>本文通过利用translation embedding的思想将主语物体的特征与宾语物体的特征之间的差与它们之间的谓语关系建立等式，进行关系检测的建模。</p>
</li>
<li><p>Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, and Shih-Fu Chang. 2017. PPR-FCN:Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN. In IEEE International Conference on Computer Vision. IEEE.<br>本文提出了一个基于成对的区域的并行全卷积神经网络，解决弱监督的视觉关系检测任务。</p>
</li>
<li><p>Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics 1 (2013), 25–36.<br>本文针对视频中发生的动作提出了一个具有多模态的语料库，为比较动作短语的相似性建立了标准的数据集并报告了有关视频中动作相似性的·实验结果。</p>
</li>
<li><p>C Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the visual interpretation of sentences. In IEEE International Conference on Computer Vision. IEEE, 1681–1688.<br>通过对句子进行关系三元组的提取，并对提取之后的这些元组进行特征学习，来解决场景生成和场景索引的任务。</p>
</li>
</ol>
<h2 id="Action-recognition"><a href="#Action-recognition" class="headerlink" title="Action recognition"></a>Action recognition</h2><ol start="22">
<li><p>An-An Liu, Yu-Ting Su,Wei-Zhi Nie, and Mohan Kankanhalli. 2017. Hierarchical clustering multi-task learning for joint human action grouping and recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 1 (2017), 102–114.<br>本文针对人类动作分组和识别任务，提出了一个分层群聚的多任务学习方法。</p>
</li>
<li><p>Li Niu, Xinxing Xu, Lin Chen, Lixin Duan, and Dong Xu. 2016. Action and event recognition in videos by learning from heterogeneous web sources. IEEE Transactions on Neural Networks and Learning Systems (2016).<br>本文通过对多种异质资源提取特征并进行融合，来训练出鲁棒的分类器。</p>
</li>
<li><p>Yan Yan, Elisa Ricci, Ramanathan Subramanian, Gaowen Liu, and Nicu Sebe. 2014. Multitask linear discriminant analysis for view invariant action recognition. IEEE Transactions on Image Processing 23, 12 (2014), 5599–5611.<br>本文通过对人类动作的多个视角的特征进行提取，然后用多任务的线性判别分析框架对提取到的特征进行学习，从而实现对人类动作的识别。</p>
</li>
<li><p>Yu-Gang Jiang, Qi Dai, Xiangyang Xue, Wei Liu, and Chong-Wah Ngo. 2012. Trajectory-based modeling of human actions with motion reference points. In European Conference on Computer Vision. Springer, 425–438.<br>本文采用全局和局部参考点来特征化移动信息，对相机移动有较好的鲁棒性，同时将物体间的关系考虑进去，产生了较好的行为特征表示。</p>
</li>
<li><p>Heng Wang and Cordelia Schmid. 2013. Action recognition with improved trajectories. In IEEE International Conference on Computer Vision. IEEE, 3551– 3558.<br>本文通过显式地估计相机移动来改善密集轨迹。</p>
</li>
<li><p>Karen Simonyan and Andrew Zisserman. 2014. Two-stream convolutional networks for action recognition in videos. In Advances in Neural Information Processing Systems. 568–576.<br>本文首次提出two stream网络，主要分为两个流，空间流处理静止图像帧，得到形状信息，时间流处理连续多帧稠密光流，得到运动信息。</p>
</li>
</ol>
<ol start="28">
<li>LiminWang, Yu Qiao, and Xiaoou Tang. 2015. Action recognition with trajectory-pooled deep-convolutional descriptors. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 4305–4314.<br>本文在iDT和two-stream ConvNets的基础上，对提取出来的卷积特征图实施受限于轨迹的采样与池化来获得轨迹池化的深度卷积描述子。</li>
</ol>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/05/26/survey-about-video-related-work-in-computer-vision/">
                survey about video-related work in computer vision
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-05-26</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <ol>
<li>Zheng Shou, Dongang Wang, Shih-Fu Chang:Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs. CVPR 2016: 1049-1058</li>
<li>Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh:Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? CoRR abs/1711.09577 (2017)</li>
<li>Colin Lea, Michael D. Flynn, René Vidal, Austin Reiter, Gregory D. Hager:Temporal Convolutional Networks for Action Segmentation and Detection. CVPR 2017: 1003-1012</li>
<li>Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri:Learning Spatiotemporal Features with 3D Convolutional Networks. ICCV 2015: 4489-4497</li>
<li>Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, Shih-Fu Chang:CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos. CVPR 2017: 1417-1426</li>
<li>Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui Liu, Xiaogang Wang:Object Detection in Videos with Tubelet Proposal Networks. CVPR 2017: 889-897</li>
<li>Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, Dahua Lin:Temporal Action Detection with Structured Segment Networks. ICCV 2017: 2933-2942</li>
<li>Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei:Flow-Guided Feature Aggregation for Video Object Detection. ICCV 2017: 408-417</li>
<li>Yuan, Xiaodan Liang, Xiaolong Wang, Dit-Yan Yeung, Abhinav Gupta:Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection. ICCV 2017: 1819-1828</li>
<li>Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei:Deep Feature Flow for Video Recognition. CVPR 2017: 4141-4150</li>
<li>Khurram Soomro, Mubarak Shah:Unsupervised Action Discovery and Localization in Videos. ICCV 2017: 696-705</li>
<li>Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, Luc Van Gool:Temporal Segment Networks: Towards Good Practices for Deep Action Recognition. ECCV (8) 2016: 20-36</li>
<li>Ke Yang, Peng Qiao, Dongsheng Li, Shaohe Lv, Yong Dou:Exploring Temporal Preservation Networks for Precise Temporal Action Localization. AAAI 2018</li>
<li>Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, Bernard Ghanem:DAPs: Deep Action Proposals for Action Understanding. ECCV (3) 2016: 768-784</li>
<li>Rui Hou, Chen, Mubarak Shah:Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos. ICCV 2017: 5823-5832</li>
<li>Yicong Tian, Rahul Sukthankar, Mubarak Shah:Spatiotemporal Deformable Part Models for Action Detection. CVPR 2013: 2642-2649</li>
<li>Xiaojiang Peng, Cordelia Schmid:Multi-region Two-Stream R-CNN for Action Detection. ECCV (4) 2016: 744-759</li>
<li>Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei:End-to-End Learning of Action Detection from Frame Glimpses in Videos. CVPR 2016: 2678-2687</li>
<li>Shugao Ma, Jianming Zhang, Nazli Ikizler-Cinbis, Stan Sclaroff:Action Recognition and Localization by Hierarchical Space-Time Segments. ICCV 2013: 2744-2751</li>
<li>Dan Oneata, Jakob J. Verbeek, Cordelia Schmid:Efficient Action Localization with Approximately Normalized Fisher Vectors. CVPR 2014: 2545-2552</li>
</ol>
<p>[1]提出了一种用于时序动作定位的多阶段3D卷积网络，包括多尺度视频片段的生成、多阶段segment-CNN以及后处理（temporal action localization）。<br>[2]观察到深度2D卷积神经网络经过ImageNet数据集预训练后迁移到detection，segmentation，captioning等其他任务上取得的显著成就后，尝试在Kinetics数据集上对深度3D卷积神经网络预训练，观察迁移到detection、summarization以及Optical flow的提取等任务上的效果（action recognition）。<br>[3]提出了一个基于时序卷积的卷积-逆卷积网络，来实现对视频帧的类别分类（action segmentation and detection）。<br>[4]提出了一种用于提取视频时空特征的3D卷积神经网络（action recognition）。<br>[5]基于C3D提出了一种卷积-逆卷积的神经网络，输入一小段视频，输出frame-level的动作类别概率和segment-level的时序动作定位（temporal action localization）。<br>[6]将传统针对静止图像的FasterRCNN框架进行了扩展，将视频物体检测框架扩展为候选时空管道生成和候选时空管道识别两个模块，基于这些高质量的候选时空管道应用编码-解码LSTM网络进行时空管道的识别能够有效的提升检测整体的正确率（video object detection）。<br>[7]提出了一种结构化分段的时序金字塔网络，既产生高质量的动作提议又提升了对动作的准确识别率和定位（temporal action localization）。<br>[8]提出使用时间域的聚合提升单帧特征学习，使得对视频中每一帧的物体识别率得到提高（video object detection）。<br>[9]提出了一种以动作描述驱动的时序动态图网络（video object detection）。<br>[10]利用视频中连续帧内容高度相关这一特点只对关键帧计算CNN特征提取，然后通过一个flow field将关键帧的CNN特征propagte到其他帧去，减少了计算量（video object detection）。<br>[11]提出了一种对视频动作进行聚类、标记以及定位的无监督方法（temporal action localization）。<br>[12]提出一种时序分段网络，并基于two-stream法进行改进（action recognition）。<br>[13]基于[5]提出TPN（temporal preservation networks）模型，将temporal convolution替换为temporal preservation convolution能够在不进行时序池化操作的情况下获得同样大小的感受野而不缩短时序长度，从而更好地保留时序信息（temporal action localization）。<br>[14]提出了一种产生动作提议的高效算法（temporal action localization）。<br>[15]提出一种管道卷积神经网络，先将一个视频分成clips，分别从clips中产生候选动作管道，然后这些clips中的候选管道进行合理地衔接，最后对这些衔接好的候选管道进行动作检测（temporal action detection）。<br>[16]提出了一种时空可变部分模型（SDPM），对每类动作学习出一种时空模式，也就是将这种时空模式与目标动作进行尽可能地匹配（temporal action localization）。<br>[17]提出一种多区域的two-stream Faster R-CNN网络，在appearance R-CNN和motion R-CNN中选择多个身体区域（即上半身、下半身和边界区域）来提高帧级动作检测的性能（temporal action localization）。<br>[18]提出了一种通过预测待处理的下一帧位置来逐步完成对视频的动作检测（temporal action localization）。<br>[19]提出一种对视频帧的层级式的分割表示方法，第一层表示出该视频帧的最基本的分割（root），第二层则更细粒度地将root进行分割表示（temporal action localization）。<br>[20]基于对图片的费舍向量表示提出了对视频的近似的正则化费舍向量表示方法（temporal action localization）。</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/04/20/Temporal-Action-Detection-with-Structured-Segment-Networks笔记/">
                Temporal Action Detection with Structured Segment Networks笔记
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-04-20</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>先说说这篇<a href="https://arxiv.org/pdf/1704.06228" title="下载" target="_blank" rel="noopener">论文</a>主要研究什么？<br>简而言之，就是对视频中出现的行为进行检测，目标是预测行为的类别和行为所在的时序区间。<br>本文提出了一种结构化的分段网络，这样更容易提炼出视频中关于行为的信息，并采用TAG（temporal actionness grouping)来产生高质量的行为提议。下面来分别对这两种新点子讲讲自己的理解：<br>Structured Segment Network:<br><a href="https://photouploads.com/image/EjvY" target="_blank" rel="noopener"><img src="http://photouploads.com/images/baf323.png" alt="baf323.png"></a><br>该网络采用了一种结构化的时序金字塔池化方法（STPP）对提议（proposal）进行提取特征。首先将原始视频进行分帧，然后用TAG方法产生许多proposal。对于每一个proposal，对其进行增广后，文中提出将其分割成L个段（segment），然后从这L个段中分别采样出一个片段（snippet）来。这一个snippet是由一些连续的帧和这些帧对应的光流图像（一个帧对应到许多光流图像，光流图像是另外从视频中计算提取）组成，即是单帧表示静态内容，许多光流图像表示对应的动态内容。这样的话就产生了L个snippets，而增广提议被分割成三个阶段（starting，course，ending)，这样每个阶段都包含了一定数目的snippets，总和加起来是L。 具体来说，文中提出对这三个阶段的处理都不同。对于course阶段，文中提出把它分两级，第一级就是一个部分，第二级分为两个部分。对于starting和ending的阶段，直接就是一级，这样的话总共就是5个部分。然后将这5个部分里面的snippet送入特征提取器（feature extractor)，得到Vt。对于代表每个部分的池化特征，文中提出这样计算：<br><img src="http://photouploads.com/images/d8659d.png" alt="d8659d.png"><br>最后将所有的池化特征进行联结，得到这个代表整个增广提议的全局特征，送入两种分类器中。这两种分类器一类是多类行为分类器，另一类是一系列二元完整性分类器。多类行为分类器对视频中的行为分为K+1类，K种行为外加一个背景类，这种分类器只对course阶段的三个部分所提取联结成的全局特征进行处理。而一系列（K个）二元完整性分类器是对所有部分所提取联结成的全局特征进行处理。这两种分类器的结果进行综合便得到行为所属的类别。<br><a href="https://photouploads.com/image/EjvW" target="_blank" rel="noopener"><img src="http://photouploads.com/images/34a543.png" alt="34a543.png"></a><br>我解释下上述的计算分类loss的公式，第一项是在给定提议pi下，求该提议中出现行为类别ci（ci属于K+1个类别之一）的概率，进行交叉熵计算。然后第二项只对非背景类进行计算，在给定提议pi下并且确定了其中包含的行为类别ci，求该提议下行为是完整（bi用来表示行为是完整的）的概率，进行交叉熵计算。最后把这两项交叉熵加起来得到总的分类损失。对于用TAG产生的proposals，文中提出将其分为三类标签，分别是（ci&gt;0,bi=1)，ci=0，(ci&gt;0,bi=0)，也就是提议中出现行为并且行为是完整的、提议中没有出现行为即只出现背景、提议中出现行为但不完整（具体怎么打这三个标签得依据最近的groundtruth）。在训练的时候，实际上这个分类损失就是网络模型对proposal预测的值（提议中出现某类行为的概率）与该proposal本身的标签之间进行比较而来的。<br><a href="https://photouploads.com/image/Ejva" target="_blank" rel="noopener"><img src="http://photouploads.com/images/00d043.png" alt="00d043.png"></a><br>对于行为的定位，文中提出与RCNN中盒回归类似的时序回归方法即是对正提议pi（positive proposal，ci&gt;0,bi=1）的中心ui，和长度θi（log尺度）相对于groundtruth的变化进行回归，这个groundtruth指的是与该正提议最接近的groundtruth 。至于回归函数，文中提出用smooth L1损失函数，然后将这两类损失进行相加得到上述的多任务损失。用λ来调整两个损失的重要性。</p>
<p>下面再讲讲TAG（temporal actionness grouping，时序行为性分组）技术的原理。首先这个技术是用来产生proposal的，相比于其他产生proposal的方法，这个技术产生的数量更少并且更好。<br>      <a href="https://photouploads.com/image/Ejvi" target="_blank" rel="noopener"><img src="http://photouploads.com/images/188580.png" alt="188580.png"></a><br>      首先用一个二元行为性分类器对从视频中提取出的一系列snippets进行分类，得到结果就如上图蓝色线所示。然后倒过来得到红色线，用分水岭算法对红色线所示的低洼进行处理，即是用不同级别γ的水对低洼进行灌溉。低洼越低，所在的水级颜色越深。最后把这些流域（basins）G(γ)提出来得到红线之下的方块图。然后我再说说产生proposal的方块合并策略，从第一个方块开始，逐渐加入紧随着的方块，若用方块的底边之和除以第一个方块的左边界到最后加入的方块的右边界之间的长度所计算出来的值低于阈值T时，第一块的左边界到最后加入的一块的右边界之间的段作为一个proposal。文中提出从0到1之间以0.05的步长来设置一系列γ和T，用这些参数采用TAG方法产生出proposals，最后用NMS（非极大值抑制）方法去除高度重合的proposal来得到最终的proposals。</p>
<p>总结一下，文中提出的这种对proposal增广又分级进行处理的策略着实有效，TAG技术的运用使得proposal的质量大大提高，具体细节请看原文（在第一行“论文”两字上），想要交流的可在下面评论^_^                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/04/15/新的开始/">
                新的开始
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-04-15</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>费了好几天，终于把自己的网站弄好了。之前一直用CSDN托管的<a href="https://blog.csdn.net/qq_22194315" title="my old blog" target="_blank" rel="noopener">傻瓜式博客</a>，感觉没多大意思。所以想自己建个网站，由于github免费提供服务器托管网页，那就直接考虑git+hexo的方式建站。</p>
<p>然后自己看了看廖雪峰的<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">git教程</a>，受益颇多。然后根据<a href="https://blog.csdn.net/qq_22194315/article/details/79936134" target="_blank" rel="noopener">这篇博客</a>就搭建了现在这个网站，感觉良好:)</p>
<p>至于为什么要写博客，我列如下几个原因：</p>
<ul>
<li>感觉学习效率不高，用博客来记录学习历程应该很有趣并可以提高学习的效率；</li>
<li>写博客的话会让生活更加充实，我有幸看到一位大牛的<a href="http://pluskid.org/" target="_blank" rel="noopener">博客</a>，并不是想盲目模仿别人而是觉得这种方式的确会让生活不会太无聊；</li>
<li>因为自己有继续深造的想法，那么好好打造自己应该先从一个blogger开始。</li>
</ul>
<p>对于这个新的博客，我想多写写自己的东西，而不是一昧地去转载别人的。<br>之前的CSDN博客就是这样的，自己的东西还是少，主要是收藏别人的东西。</p>
<p>对于要写的东西，我做了如下几点分类：</p>
<ul>
<li>与自己研究方向相关的东西</li>
<li>在生活中遇到的一些有趣的事</li>
<li>对人生和世界的思考</li>
</ul>
<p>最后以一幅图结尾，</p>
<p> <img src="http://pic.sc.chinaz.com/files/pic/pic9/201510/apic15654.jpg" alt="icon"></p>
<p>只希望品人生百味，或喜或悲，前途路上处处皆风景！</p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2018/04/13/hello-world/">
                Hello World
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2018-04-13</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

        </div>
    

</div>
            
        </section>
    </div>
</div>






</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2018/08/05/survey-about-video-visual-relation-detection-in-computer-vision/">survey-about-video-visual</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/05/26/survey-about-video-related-work-in-computer-vision/">survey about video-relate</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/04/20/Temporal-Action-Detection-with-Structured-Segment-Networks笔记/">Temporal Action Detection</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2018/04/15/新的开始/">新的开始</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/qazwsx74269">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    
                    
                    
                    
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:songzehan19941109@gmail.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @SZH. All right reserved ！！！| Design & Hexo <a href="http://www.codeblocq.com/">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->
<script src="/js/main.js"></script>

<!-- Disqus Comments -->



</body>

</html>